{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "aa1r7fSRTwho",
    "outputId": "2fd2c201-dcfc-4905-f3f9-25a580968f1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Image-Classification-and-Deployment'...\n",
      "remote: Enumerating objects: 76, done.\u001b[K\n",
      "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 76 (delta 23), reused 72 (delta 19), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (76/76), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/entiretydotai/Image-Classification-and-Deployment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vSwksQ2F70-"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets , transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "class Cifar10Data:\n",
    "    def __init__(self,):\n",
    "        self.train_transforms = transforms.Compose([transforms.RandomCrop(size=32 , padding=4 , padding_mode=\"symmetric\",pad_if_needed=True),\n",
    "                                                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                                    transforms.ToTensor()])\n",
    "        self.val_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "        self.trainset = datasets.CIFAR10(train=True,root = \"data/\",download=True,transform=self.train_transforms)\n",
    "        self.valset  = datasets.CIFAR10(train=False , root=\"data/\",download=True,transform=self.val_transforms)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def dataloader(self,batch_size=64 ,num_workers = 4,device_count=torch.cuda.device_count()):\n",
    "\n",
    "        loader_param = { \"batch_size\":batch_size*device_count,\n",
    "                        \"pin_memory\":True,\n",
    "                        \"num_workers\":num_workers,\n",
    "                        \"shuffle\":True}\n",
    "\n",
    "        trainLoader = DataLoader(self.trainset,**loader_param)\n",
    "        valLoader = DataLoader(self.valset  ,**loader_param)\n",
    "        return {\"train\":trainLoader , \"val\":valLoader}\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self,): return len(self.trainset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MMTyLCYGCux"
   },
   "outputs": [],
   "source": [
    "#from .densenet import DenseNet\n",
    "import torch\n",
    "\n",
    "def getModel(training=False,**kwargs):\n",
    "\n",
    "    device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DenseNet(**kwargs)\n",
    "    model.eval()\n",
    "    if training :\n",
    "        model.train()\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        print(\"The model is in training mode\")\n",
    "    print(\"No of params in model is \" , sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    model  = model.to(device)\n",
    "    print(f\"model is loaded on GPU {next(model.parameters()).is_cuda}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jkgkSyI_GD6u"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self,num_channels,growth_rate,bn_size,drop_rate):\n",
    "        super(DenseLayer,self).__init__()\n",
    "        mid_channel = int(growth_rate*bn_size)\n",
    "        self.add_module(\"bn1\",nn.BatchNorm2d(num_channels))\n",
    "        self.add_module(\"relu1\",nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv1\",nn.Conv2d(num_channels,mid_channel ,kernel_size=1 , bias=False))\n",
    "        self.add_module(\"bn2\",nn.BatchNorm2d(mid_channel))\n",
    "        self.add_module(\"relu2\",nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv2\",nn.Conv2d(mid_channel ,growth_rate,kernel_size=3,padding=1 , bias=False))\n",
    "        self.drop_rate=drop_rate\n",
    "    def forward(self,*prev_features):\n",
    "        concated_features = torch.cat(prev_features, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.bn1(concated_features)))\n",
    "        new_features = self.conv2(self.relu2(self.bn2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return new_features\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self,num_channels,num_out_channels):\n",
    "        super(Transition,self).__init__()\n",
    "        self.add_module(\"bn\",nn.BatchNorm2d(num_channels))\n",
    "        self.add_module(\"relu\",nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv\",nn.Conv2d(num_channels,num_out_channels ,kernel_size=1 , bias=False))\n",
    "        self.add_module(\"pool\",nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "    def forward(self,x):\n",
    "        out = self.conv(self.relu(self.bn(x)))\n",
    "        out = self.pool(out)\n",
    "        return out\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_layers,num_channels,growth_rate,bn_size,drop_rate):\n",
    "        super(DenseBlock,self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(num_channels=num_channels+i*growth_rate,\n",
    "                               growth_rate=growth_rate,\n",
    "                               bn_size=bn_size,\n",
    "                               drop_rate=drop_rate)\n",
    "            self.add_module(f\"denselayer{i+1}\",layer)\n",
    "    \n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,growth_rate=32,block_config=(6,12,24,16),\n",
    "                num_init_features=64,bn_size=4, drop_rate=0.1,num_classes=10):\n",
    "        super(DenseNet,self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            (\"conv0\",nn.Conv2d(3,num_init_features,kernel_size=3,bias=False)),\n",
    "        ]))\n",
    "        \n",
    "        num_features=num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block  = DenseBlock(num_layers = num_layers,\n",
    "                               num_channels=num_features,\n",
    "                                growth_rate=growth_rate,\n",
    "                                bn_size=bn_size,\n",
    "                                drop_rate=drop_rate)\n",
    "            self.features.add_module(f\"denseblock{i+1}\",block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i<len(block_config)-1:\n",
    "                transition = Transition(num_features,num_features//2)\n",
    "                num_features=num_features//2\n",
    "                self.features.add_module(f\"transition{i+1}\",transition)\n",
    "        self.features.add_module(\"norm5\",nn.BatchNorm2d(num_features))\n",
    "        self.classifier = nn.Linear(num_features,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1TAZfs1GEBB"
   },
   "outputs": [],
   "source": [
    "#from ..model import getModel\n",
    "#from ..data import Cifar10Data\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "def check_for_dir(*args_path):\n",
    "    for path in args_path:\n",
    "        if not path.exists():\n",
    "            path.mkdir(parents=True)\n",
    "\n",
    "def delete_file(path):\n",
    "    if path.exists() and not path.is_dir():\n",
    "        path.unlink()\n",
    "\n",
    "def logger(filepath , *args,**kwargs):\n",
    "    print(*args,**kwargs)\n",
    "    with open(filepath,\"a\") as f:  # appends to file and closes it when finished\n",
    "        print(file=f,*args,**kwargs)\n",
    "\n",
    "def train_model(model,\n",
    "          data_loader ,\n",
    "          optimizer=None,\n",
    "          criterion =None,\n",
    "          num_epochs=5 ,\n",
    "          save_model_filename=\"saved_weights.pt\",\n",
    "          log_filename=\"training_logs.txt\"):\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    if criterion is None:\n",
    "        criterion =torch.nn.CrossEntropyLoss()\n",
    "    global logger\n",
    "    log_filename_path = Path(\"/content/Image-Classification-and-Deployment/src/training_logs/\")\n",
    "    save_model_filename_path = Path(\"/content/Image-Classification-and-Deployment/src/saved_weights/\")\n",
    "    check_for_dir(log_filename_path,save_model_filename_path)\n",
    "    save_model_filename_path = save_model_filename_path/save_model_filename\n",
    "    log_filename_path = log_filename_path/log_filename\n",
    "    delete_file(log_filename_path)\n",
    "    logger = partial(logger,log_filename_path)    \n",
    "    device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in trange(num_epochs,desc=\"Epochs\"):\n",
    "        result = [f\"[ Epochs {epoch} | {num_epochs} ] : \"]\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase==\"train\":     # put the model in training mode\n",
    "                model.train()\n",
    "            else:     # put the model in validation mode\n",
    "                model.eval()\n",
    "\n",
    "            # keep track of training and validation loss\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            for data , target in data_loader[phase]:\n",
    "                #load the data and target to respective device\n",
    "                data , target = data.to(device)  , target.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(phase==\"train\"):\n",
    "                    #feed the input\n",
    "                    output = model(data)\n",
    "                    #calculate the loss\n",
    "                    loss = criterion(output,target)\n",
    "                    preds = torch.argmax(output,1)\n",
    "\n",
    "                if phase==\"train\"  :\n",
    "                    # backward pass: compute gradient of the loss with respect to model parameters \n",
    "                    loss.backward()\n",
    "                    # update the model parameters\n",
    "                    optimizer.step()\n",
    "                    # zero the grad to stop it from accumulating\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                running_corrects += torch.sum(preds == target.data).item()\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(data_loader[phase].dataset)\n",
    "            if phase ==\"val\":\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    logger(f\"Saving the current best model. Previous best loss = {best_val_loss} Current best loss = {epoch_loss}\")\n",
    "                    best_val_loss = epoch_loss\n",
    "                    torch.save(model.module.state_dict(),save_model_filename_path)\n",
    "            result.append('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "        logger(\" \".join(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JxSbNJDGD-T"
   },
   "outputs": [],
   "source": [
    "#from .model import getModel\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import torch\n",
    "from torchvision import  transforms\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, save_model_filename=\"saved_weights.pt\"):\n",
    "        self.classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship','truck']\n",
    "        self.model = getModel(training=False,num_classes=len(self.classes))\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.load_state_dict(torch.load(f\"/content/Image-Classification-and-Deployment/src/saved_weights/{save_model_filename}\",map_location=device))\n",
    "        return None\n",
    "\n",
    "    def __call__(self,image):\n",
    "        if isinstance(image,(Path,str)):\n",
    "            image = PIL.Image.open(image).convert(\"RGB\")\n",
    "        elif not isinstance(image,PIL.JpegImagePlugin.JpegImageFile): \n",
    "            raise Exception(\"must be PIL image or path \")\n",
    "        image_input = transforms.ToTensor()(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            out = self.model(image_input).squeeze(0)\n",
    "            prob = torch.argmax(out).item()\n",
    "            return self.classes[prob]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDL95hJkTeFA"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets , transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "class Cifar10Data:\n",
    "    def __init__(self,):\n",
    "        self.train_transforms = transforms.Compose([transforms.RandomCrop(size=32 , padding=4 , padding_mode=\"symmetric\",pad_if_needed=True),\n",
    "                                                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                                    transforms.ToTensor()])\n",
    "        self.val_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "        self.trainset = datasets.CIFAR10(train=True,root = \"data/\",download=True,transform=self.train_transforms)\n",
    "        self.valset  = datasets.CIFAR10(train=False , root=\"data/\",download=True,transform=self.val_transforms)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def dataloader(self,batch_size=64 ,num_workers = 4,device_count=torch.cuda.device_count()):\n",
    "        loader_param = { \"batch_size\": batch_size, #batch_size*device_count,\n",
    "                        \"pin_memory\":True,\n",
    "                        \"num_workers\":num_workers,\n",
    "                        \"shuffle\":True}\n",
    "\n",
    "        trainLoader = DataLoader(self.trainset,**loader_param)\n",
    "        valLoader = DataLoader(self.valset  ,**loader_param)\n",
    "        return {\"train\":trainLoader , \"val\":valLoader}\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self,): return len(self.trainset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "a0de8a652ad4482ebe58ac8c864389f5",
      "2d8557ef5ab04f38a1edd0c8e2a537a3",
      "5ff995e645474884b1a43e937c619ad2",
      "27d40bc8804f41a889c4514d5c3c967b",
      "65109a3bd3cf423b8248ec7904471a76",
      "81811aad8c9d4a43868ef0e3b633fdbd",
      "a5eaad2ceb4a48879dacda1af7e4e773",
      "345ef314eda24e1a90d57fbcfb3660f1"
     ]
    },
    "colab_type": "code",
    "id": "GuJp6IcnTTAZ",
    "outputId": "0fc027ea-80e4-4820-c633-01f10234a4e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0de8a652ad4482ebe58ac8c864389f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data = Cifar10Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Fz7NlQB2TuWy",
    "outputId": "49656040-3cce-4b0f-ae1a-fcc28ed6af99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is in training mode\n",
      "No of params in model is  6956298\n",
      "model is loaded on GPU True\n"
     ]
    }
   ],
   "source": [
    "model = getModel(training=True,num_classes=data.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kCm9Ho7sTwWt",
    "outputId": "0bda6f67-3cab-4179-91fc-9beb0385bef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:   0%|          | 0/70 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Epochs:   1%|▏         | 1/70 [03:52<4:27:38, 232.73s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = inf Current best loss = 0.7724563220977784\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 0 | 70 ] :  train Loss: 0.9332 Acc: 0.6695 val Loss: 0.7725 Acc: 0.7389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:   3%|▎         | 2/70 [07:45<4:23:51, 232.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.7724563220977784 Current best loss = 0.6927881395339965\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 1 | 70 ] :  train Loss: 0.6854 Acc: 0.7601 val Loss: 0.6928 Acc: 0.7664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:   4%|▍         | 3/70 [11:38<4:20:05, 232.92s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.6927881395339965 Current best loss = 0.5283596396446228\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 2 | 70 ] :  train Loss: 0.5630 Acc: 0.8050 val Loss: 0.5284 Acc: 0.8223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:   6%|▌         | 4/70 [15:32<4:16:16, 232.98s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 3 | 70 ] :  train Loss: 0.4865 Acc: 0.8320 val Loss: 0.5305 Acc: 0.8214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:   7%|▋         | 5/70 [19:25<4:12:26, 233.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.5283596396446228 Current best loss = 0.44283470888137816\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 4 | 70 ] :  train Loss: 0.4417 Acc: 0.8476 val Loss: 0.4428 Acc: 0.8518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:   9%|▊         | 6/70 [23:18<4:08:46, 233.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.44283470888137816 Current best loss = 0.40042409048080446\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 5 | 70 ] :  train Loss: 0.3932 Acc: 0.8628 val Loss: 0.4004 Acc: 0.8685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  10%|█         | 7/70 [27:13<4:05:13, 233.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.40042409048080446 Current best loss = 0.39202661867141725\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 6 | 70 ] :  train Loss: 0.3578 Acc: 0.8766 val Loss: 0.3920 Acc: 0.8722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  11%|█▏        | 8/70 [31:07<4:01:34, 233.78s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 7 | 70 ] :  train Loss: 0.3294 Acc: 0.8858 val Loss: 0.4416 Acc: 0.8569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  13%|█▎        | 9/70 [35:00<3:57:26, 233.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.39202661867141725 Current best loss = 0.36194811894893647\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 8 | 70 ] :  train Loss: 0.3037 Acc: 0.8952 val Loss: 0.3619 Acc: 0.8812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  14%|█▍        | 10/70 [38:54<3:53:33, 233.56s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 9 | 70 ] :  train Loss: 0.2762 Acc: 0.9050 val Loss: 0.3992 Acc: 0.8730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  16%|█▌        | 11/70 [42:48<3:49:48, 233.71s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.36194811894893647 Current best loss = 0.3420346430897713\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 10 | 70 ] :  train Loss: 0.2612 Acc: 0.9085 val Loss: 0.3420 Acc: 0.8909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  17%|█▋        | 12/70 [46:41<3:45:56, 233.73s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 11 | 70 ] :  train Loss: 0.2492 Acc: 0.9126 val Loss: 0.3587 Acc: 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  19%|█▊        | 13/70 [50:35<3:42:03, 233.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.3420346430897713 Current best loss = 0.33475141224861144\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 12 | 70 ] :  train Loss: 0.2263 Acc: 0.9213 val Loss: 0.3348 Acc: 0.8957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  20%|██        | 14/70 [54:29<3:38:08, 233.72s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.33475141224861144 Current best loss = 0.3083544547557831\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 13 | 70 ] :  train Loss: 0.2072 Acc: 0.9273 val Loss: 0.3084 Acc: 0.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  21%|██▏       | 15/70 [58:22<3:34:13, 233.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 14 | 70 ] :  train Loss: 0.1974 Acc: 0.9308 val Loss: 0.3141 Acc: 0.9024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  23%|██▎       | 16/70 [1:02:16<3:30:22, 233.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 15 | 70 ] :  train Loss: 0.1830 Acc: 0.9356 val Loss: 0.3139 Acc: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  24%|██▍       | 17/70 [1:06:10<3:26:23, 233.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.3083544547557831 Current best loss = 0.3062785828113556\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 16 | 70 ] :  train Loss: 0.1732 Acc: 0.9392 val Loss: 0.3063 Acc: 0.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  26%|██▌       | 18/70 [1:10:03<3:22:26, 233.59s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.3062785828113556 Current best loss = 0.28549523363113405\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 17 | 70 ] :  train Loss: 0.1622 Acc: 0.9427 val Loss: 0.2855 Acc: 0.9106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  27%|██▋       | 19/70 [1:13:57<3:18:34, 233.61s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 18 | 70 ] :  train Loss: 0.1516 Acc: 0.9460 val Loss: 0.3007 Acc: 0.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  29%|██▊       | 20/70 [1:17:51<3:14:40, 233.62s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 19 | 70 ] :  train Loss: 0.1426 Acc: 0.9493 val Loss: 0.3127 Acc: 0.9065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  30%|███       | 21/70 [1:21:44<3:10:46, 233.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 20 | 70 ] :  train Loss: 0.1346 Acc: 0.9520 val Loss: 0.2986 Acc: 0.9129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  31%|███▏      | 22/70 [1:25:38<3:06:56, 233.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 21 | 70 ] :  train Loss: 0.1290 Acc: 0.9547 val Loss: 0.2927 Acc: 0.9181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  33%|███▎      | 23/70 [1:29:32<3:03:12, 233.88s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 22 | 70 ] :  train Loss: 0.1222 Acc: 0.9562 val Loss: 0.2870 Acc: 0.9190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  34%|███▍      | 24/70 [1:33:27<2:59:25, 234.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 23 | 70 ] :  train Loss: 0.1160 Acc: 0.9592 val Loss: 0.3098 Acc: 0.9103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  36%|███▌      | 25/70 [1:37:21<2:55:29, 233.98s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 24 | 70 ] :  train Loss: 0.1075 Acc: 0.9615 val Loss: 0.2960 Acc: 0.9168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  37%|███▋      | 26/70 [1:41:14<2:51:34, 233.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 25 | 70 ] :  train Loss: 0.1039 Acc: 0.9636 val Loss: 0.3080 Acc: 0.9112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  39%|███▊      | 27/70 [1:45:08<2:47:34, 233.82s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 26 | 70 ] :  train Loss: 0.1006 Acc: 0.9643 val Loss: 0.2932 Acc: 0.9223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  40%|████      | 28/70 [1:49:02<2:43:43, 233.89s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 27 | 70 ] :  train Loss: 0.0947 Acc: 0.9674 val Loss: 0.3006 Acc: 0.9197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  41%|████▏     | 29/70 [1:52:56<2:39:55, 234.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 28 | 70 ] :  train Loss: 0.0890 Acc: 0.9687 val Loss: 0.3000 Acc: 0.9217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  43%|████▎     | 30/70 [1:56:50<2:35:59, 233.98s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 29 | 70 ] :  train Loss: 0.0881 Acc: 0.9693 val Loss: 0.3191 Acc: 0.9166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  44%|████▍     | 31/70 [2:00:44<2:32:04, 233.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 30 | 70 ] :  train Loss: 0.0854 Acc: 0.9693 val Loss: 0.3086 Acc: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  46%|████▌     | 32/70 [2:04:38<2:28:11, 233.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 31 | 70 ] :  train Loss: 0.0752 Acc: 0.9731 val Loss: 0.3135 Acc: 0.9157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  47%|████▋     | 33/70 [2:08:33<2:24:24, 234.19s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 32 | 70 ] :  train Loss: 0.0751 Acc: 0.9734 val Loss: 0.3053 Acc: 0.9243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  49%|████▊     | 34/70 [2:12:27<2:20:30, 234.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 33 | 70 ] :  train Loss: 0.0722 Acc: 0.9742 val Loss: 0.3093 Acc: 0.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  50%|█████     | 35/70 [2:16:21<2:16:34, 234.13s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 34 | 70 ] :  train Loss: 0.0674 Acc: 0.9764 val Loss: 0.3171 Acc: 0.9203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  51%|█████▏    | 36/70 [2:20:15<2:12:42, 234.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 35 | 70 ] :  train Loss: 0.0710 Acc: 0.9750 val Loss: 0.3369 Acc: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  53%|█████▎    | 37/70 [2:24:10<2:08:49, 234.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt Saving the current best model. Previous best loss = 0.28549523363113405 Current best loss = 0.28345663957595824\n",
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 36 | 70 ] :  train Loss: 0.0663 Acc: 0.9767 val Loss: 0.2835 Acc: 0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  54%|█████▍    | 38/70 [2:28:04<2:05:00, 234.38s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 37 | 70 ] :  train Loss: 0.0585 Acc: 0.9792 val Loss: 0.3307 Acc: 0.9224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  56%|█████▌    | 39/70 [2:31:59<2:01:06, 234.41s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 38 | 70 ] :  train Loss: 0.0617 Acc: 0.9780 val Loss: 0.3600 Acc: 0.9154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  57%|█████▋    | 40/70 [2:35:53<1:57:11, 234.40s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 39 | 70 ] :  train Loss: 0.0616 Acc: 0.9782 val Loss: 0.3048 Acc: 0.9218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  59%|█████▊    | 41/70 [2:39:47<1:53:12, 234.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 40 | 70 ] :  train Loss: 0.0589 Acc: 0.9793 val Loss: 0.3245 Acc: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  60%|██████    | 42/70 [2:43:41<1:49:15, 234.14s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 41 | 70 ] :  train Loss: 0.0575 Acc: 0.9801 val Loss: 0.3117 Acc: 0.9237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  61%|██████▏   | 43/70 [2:47:35<1:45:18, 234.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 42 | 70 ] :  train Loss: 0.0546 Acc: 0.9814 val Loss: 0.3313 Acc: 0.9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  63%|██████▎   | 44/70 [2:51:28<1:41:18, 233.79s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 43 | 70 ] :  train Loss: 0.0544 Acc: 0.9808 val Loss: 0.3131 Acc: 0.9267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  64%|██████▍   | 45/70 [2:55:21<1:37:21, 233.67s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 44 | 70 ] :  train Loss: 0.0479 Acc: 0.9833 val Loss: 0.3520 Acc: 0.9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  66%|██████▌   | 46/70 [2:59:15<1:33:25, 233.54s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 45 | 70 ] :  train Loss: 0.0494 Acc: 0.9829 val Loss: 0.3462 Acc: 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  67%|██████▋   | 47/70 [3:03:08<1:29:32, 233.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 46 | 70 ] :  train Loss: 0.0495 Acc: 0.9824 val Loss: 0.3369 Acc: 0.9223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  69%|██████▊   | 48/70 [3:07:02<1:25:38, 233.55s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 47 | 70 ] :  train Loss: 0.0478 Acc: 0.9832 val Loss: 0.3296 Acc: 0.9225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  70%|███████   | 49/70 [3:10:55<1:21:44, 233.53s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 48 | 70 ] :  train Loss: 0.0456 Acc: 0.9842 val Loss: 0.3508 Acc: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  71%|███████▏  | 50/70 [3:14:49<1:17:51, 233.59s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 49 | 70 ] :  train Loss: 0.0408 Acc: 0.9861 val Loss: 0.3193 Acc: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  73%|███████▎  | 51/70 [3:18:43<1:13:59, 233.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 50 | 70 ] :  train Loss: 0.0459 Acc: 0.9839 val Loss: 0.3473 Acc: 0.9231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  74%|███████▍  | 52/70 [3:22:37<1:10:07, 233.76s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 51 | 70 ] :  train Loss: 0.0425 Acc: 0.9854 val Loss: 0.3242 Acc: 0.9248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  76%|███████▌  | 53/70 [3:26:31<1:06:17, 233.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 52 | 70 ] :  train Loss: 0.0439 Acc: 0.9846 val Loss: 0.3456 Acc: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  77%|███████▋  | 54/70 [3:30:26<1:02:24, 234.05s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 53 | 70 ] :  train Loss: 0.0397 Acc: 0.9867 val Loss: 0.3350 Acc: 0.9261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  79%|███████▊  | 55/70 [3:34:20<58:30, 234.05s/it]  \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 54 | 70 ] :  train Loss: 0.0393 Acc: 0.9858 val Loss: 0.3402 Acc: 0.9255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  80%|████████  | 56/70 [3:38:14<54:36, 234.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 55 | 70 ] :  train Loss: 0.0424 Acc: 0.9851 val Loss: 0.3121 Acc: 0.9287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  81%|████████▏ | 57/70 [3:42:08<50:42, 234.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 56 | 70 ] :  train Loss: 0.0367 Acc: 0.9875 val Loss: 0.3505 Acc: 0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  83%|████████▎ | 58/70 [3:46:02<46:49, 234.09s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 57 | 70 ] :  train Loss: 0.0391 Acc: 0.9863 val Loss: 0.3711 Acc: 0.9215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  84%|████████▍ | 59/70 [3:49:56<42:53, 233.98s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 58 | 70 ] :  train Loss: 0.0346 Acc: 0.9880 val Loss: 0.3475 Acc: 0.9267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  86%|████████▌ | 60/70 [3:53:50<39:00, 234.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 59 | 70 ] :  train Loss: 0.0355 Acc: 0.9870 val Loss: 0.3463 Acc: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  87%|████████▋ | 61/70 [3:57:44<35:07, 234.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 60 | 70 ] :  train Loss: 0.0372 Acc: 0.9867 val Loss: 0.3314 Acc: 0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  89%|████████▊ | 62/70 [4:01:38<31:13, 234.13s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 61 | 70 ] :  train Loss: 0.0338 Acc: 0.9882 val Loss: 0.3030 Acc: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  90%|█████████ | 63/70 [4:05:32<27:18, 234.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 62 | 70 ] :  train Loss: 0.0331 Acc: 0.9880 val Loss: 0.3531 Acc: 0.9251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  91%|█████████▏| 64/70 [4:09:26<23:24, 234.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 63 | 70 ] :  train Loss: 0.0312 Acc: 0.9890 val Loss: 0.3511 Acc: 0.9270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  93%|█████████▎| 65/70 [4:13:20<19:29, 233.88s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 64 | 70 ] :  train Loss: 0.0331 Acc: 0.9885 val Loss: 0.3457 Acc: 0.9285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  94%|█████████▍| 66/70 [4:17:14<15:35, 233.93s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 65 | 70 ] :  train Loss: 0.0345 Acc: 0.9881 val Loss: 0.3205 Acc: 0.9317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  96%|█████████▌| 67/70 [4:21:08<11:41, 233.95s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 66 | 70 ] :  train Loss: 0.0319 Acc: 0.9891 val Loss: 0.3428 Acc: 0.9268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  97%|█████████▋| 68/70 [4:25:01<07:47, 233.87s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 67 | 70 ] :  train Loss: 0.0288 Acc: 0.9899 val Loss: 0.3295 Acc: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs:  99%|█████████▊| 69/70 [4:28:55<03:53, 233.75s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 68 | 70 ] :  train Loss: 0.0326 Acc: 0.9884 val Loss: 0.3278 Acc: 0.9319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epochs: 100%|██████████| 70/70 [4:32:49<00:00, 233.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt /content/Image-Classification-and-Deployment/src/training_logs/training_logs.txt [ Epochs 69 | 70 ] :  train Loss: 0.0279 Acc: 0.9903 val Loss: 0.3655 Acc: 0.9304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model,Cifar10Data().dataloader(),num_epochs=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wb84zZzoTwc7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAML9hIETwj8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import PIL \n",
    "from PIL import Image\n",
    "\n",
    "with open(\"/content/Image-Classification-and-Deployment/data/test_batch\",\"rb\") as f:\n",
    "    entry = pickle.load(f,encoding=\"latin1\")\n",
    "    data = entry[\"data\"]\n",
    "    data = data.reshape(-1,3,32,32)\n",
    "    data = data.transpose((0,2,3,1))\n",
    "    for i in np.random.randint(0,9999,(10,)):\n",
    "        img = Image.fromarray(data[i])\n",
    "        img.save(f\"/content/Image-Classification-and-Deployment/data/tmp_{i}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "1W08zG_lc0bu",
    "outputId": "dc0aea26-fe37-4a3f-fbf2-dd7466a3fbdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdfbcb7f080>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(PIL.Image.open(\"/content/tmp_2385.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7un6sw1ZTwab"
   },
   "outputs": [],
   "source": [
    "# inputs = open(\"/content/tmp_2385.jpg\")\n",
    "# inputs = inputs.cuda()#, labels.cuda() # add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "hrenyvDeF8KF",
    "outputId": "a725db2f-d85e-401c-9b47-5cd49f4596c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of params in model is  6956298\n",
      "model is loaded on GPU True\n"
     ]
    }
   ],
   "source": [
    "infer = Inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Eil--J2TftJh",
    "outputId": "50046932-0ef8-4bf8-9082-bcf5fcc5ee40"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c41ed8ce6fa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/tmp_2385.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-032dd9ba70e6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-68c2da06de80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "infer(\"/content/tmp_2385.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "r6pMa7D0cKL8",
    "outputId": "837c6170-67eb-4108-e1f8-141f567072eb"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c41ed8ce6fa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/tmp_2385.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-032dd9ba70e6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-68c2da06de80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "infer(\"/content/tmp_2385.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUjPitCfeYMa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Aman_notebook.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "27d40bc8804f41a889c4514d5c3c967b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_345ef314eda24e1a90d57fbcfb3660f1",
      "placeholder": "​",
      "style": "IPY_MODEL_a5eaad2ceb4a48879dacda1af7e4e773",
      "value": " 170500096/? [00:30&lt;00:00, 12729349.60it/s]"
     }
    },
    "2d8557ef5ab04f38a1edd0c8e2a537a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "345ef314eda24e1a90d57fbcfb3660f1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ff995e645474884b1a43e937c619ad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81811aad8c9d4a43868ef0e3b633fdbd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65109a3bd3cf423b8248ec7904471a76",
      "value": 1
     }
    },
    "65109a3bd3cf423b8248ec7904471a76": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "81811aad8c9d4a43868ef0e3b633fdbd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0de8a652ad4482ebe58ac8c864389f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ff995e645474884b1a43e937c619ad2",
       "IPY_MODEL_27d40bc8804f41a889c4514d5c3c967b"
      ],
      "layout": "IPY_MODEL_2d8557ef5ab04f38a1edd0c8e2a537a3"
     }
    },
    "a5eaad2ceb4a48879dacda1af7e4e773": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
